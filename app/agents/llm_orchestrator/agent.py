from typing import List, Dict, Any, Optional
import os
import logging
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

logger = logging.getLogger("LLMOrchestrator")

class LLMOrchestratorAgent:
    """
    Real-world LLM Orchestrator using LangChain.
    Routes tasks to OpenAI (GPT-4o), Anthropic (Claude 3.5), or Local LLMs (via Ollama/vLLM).
    """
    def __init__(self):
        # Check for keys
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        
        # Initialize clients if keys exist
        self.gpt4 = ChatOpenAI(model="gpt-4o", temperature=0) if self.openai_key else None
        self.gpt35 = ChatOpenAI(model="gpt-3.5-turbo", temperature=0) if self.openai_key else None
        # self.claude = ChatAnthropic(model="claude-3-opus-20240229") if self.anthropic_key else None

    async def route_and_execute(self, task: str, context: str, strategy: str = "balanced") -> str:
        """
        Decides which model to use and executes the prompt.
        """
        model = self._select_model(strategy)
        
        if not model:
            return self._simulation_response(task, strategy)

        logger.info(f"Routing task to model: {model.model_name}")
        
        try:
            # Construct Chain
            prompt = ChatPromptTemplate.from_messages([
                ("system", "You are an expert AI assistant inside the MultiAgent Platform. Use the provided context to answer the user request."),
                ("system", "Context: {context}"),
                ("user", "{task}")
            ])
            
            chain = prompt | model | StrOutputParser()
            result = await chain.ainvoke({"task": task, "context": context})
            return result
            
        except Exception as e:
            logger.error(f"LLM execution failed: {e}")
            return f"âŒ **LLM Error**: {str(e)}"

    def _select_model(self, strategy: str):
        if not self.openai_key:
            return None
            
        if strategy == "quality":
            return self.gpt4
        elif strategy == "speed":
            return self.gpt35
        else:
            return self.gpt4 # Default to smart for now

    def _simulation_response(self, task: str, strategy: str) -> str:
        return f"""
### ðŸš§ Simulation Mode (No API Key Found)

The **LLM Orchestrator** is fully implemented with **LangChain**, but no `OPENAI_API_KEY` was detected in the environment.

**What would happen:**
1. **Strategy**: `{strategy}` selected.
2. **Model**: Would route to `GPT-4o` (Quality) or `GPT-3.5` (Speed).
3. **Prompt Construction**:
   - **System**: Expert Assistant
   - **Context**: (Provided Document Content)
   - **User Task**: "{task}"

**To Enable Real Intelligence**:
Add `OPENAI_API_KEY=sk-...` to your `.env` file or Docker environment.
"""

agent = LLMOrchestratorAgent()
